---
title: "Machine Learning Final Assignment"
author: "Jason Tan"
date: "27 April 2016"
output: html_document
---
#Executive Summary
This report will make predictions based on data collected by wearable gadgets worn by 6 individuals when exercising. The response variable, `classe` represents 5 different postures ('A' to 'E'), some of which are correct or incorrect. This paper will use all the different inputs to predict the posture adopted by the wearer. 

Due to the huge dataset and the limited computational capabilities, this research will start from the simplest method, and then scale it up with bagging methods if the accuracy is unsatisfactory. Because of the large number of features and datapoints, we expect that a complex method is unnecessary to obtain a good prediction. 

#Pre-Analysis preparation

###Loading Required Libraries
```{r, message = FALSE, warning = FALSE}
library(caret)
```

###Loading Data
Due to the large size of the data, we can afford to have training-test-validation sets. The 20 test samples will be used as validation. We will further split the 19622 training set to a 80-20 ratio to create a test set. 
```{r}
set.seed(0924)
dat <- read.csv("gaData.csv")
validation <- read.csv("pml-testing.csv")
```

###Data Cleaning
There exists many 'faulty' variables in the dataset which includes mostly NA or blank values. They will be removed with the following code. The result is 56 remaining variables.
```{r}
#Removing variables with predominant NAs
naindex <- which(apply(apply(dat, 2, is.na),2,sum) > 10000)
newdat <- dat[,-naindex]
#Removing non-meaningful timestamps, dates since they are either in non-meaningful formats or not useful for this analysis 
newdat <- newdat[,-(3:6)]
#It is obvious that variables with names "amplitude" "skewness" "kurtosis" "max" and "min" all have mostly blank values
newdat <- newdat[,-which(grepl("^skewness",names(newdat)))]
newdat <- newdat[,-which(grepl("^kurtosis",names(newdat)))]
newdat <- newdat[,-which(grepl("^max",names(newdat)))]
newdat <- newdat[,-which(grepl("^min",names(newdat)))]
newdat <- newdat[,-which(grepl("^amplitude",names(newdat)))]
dim(newdat)
```

###Data Preparation for training,  testing
```{r}
#Creating data partititions without the caret package
train_i <- sample(1:dim(newdat)[1], 0.8*dim(newdat)[1])
training <- newdat[train_i,]
testing <- newdat[-train_i,]
```

###Preparing Cross validation Parameter
```{r}
cvControl <- trainControl(method = "repeatedcv", number = 5, repeats = 2, search = "random")
```

#Machine Learning Model Selection process
Due to the extremely large number of data points and computational limits, we will start from the simplest methods and scale it up if the accuracy is unsatisfactory. The classifaction techniques under consideration, in order of complexity, are *linear discriminant analysis* > *quadratic discrimant analysis* > *XGB Boost* > *Random Forests*. More simple classification methods such as linear discriminant analysis and quadratic discriminant analysis will be used first. When the out of sample error rates are more than 10%, we will continue to the next simplest method.  

It is important to note that the validation set, which consist of 20 samples, will only be used to test the final selected model, otherwise, we would be unwittingly performing "training" on our validation set. Furthermore, we already have a large test set of 3925 observations.

###Linear Discriminant Analysis

After Cross Validation, out of sample error from repeated sampling within the training data is at **0.7449**. 

```{r, cache = TRUE}
ldamod <- train(classe ~ . -X,trControl = cvControl, data = training,  method = 'lda')
print(ldamod)
```


The test accuracy, on the other hand, the test error rate from `ldaconf` is 0.749. This is unsatisfactory. The unsatisfactory error rate from both cross validation and testing is expected because lda assumes homoscedacity, which is clearly false.  

```{r}
ldapred <- predict(ldamod, testing)
ldaconf <- table(testing$classe, ldapred)
print(ldaconf)
sum(diag(ldaconf))/sum(ldaconf)
```

###Quadratic Discrimant Analysis
QDA will now be used. In QDA, the homoscedacity assumption is omitted, hence likely leading to a better prediction. 

The out of sample accuracy from repeated cross validation when training has improved remarkably to **0.915**. This provides an indication that QDA is an appropriate model to use in this context. 

```{r, cache = TRUE}
qdamod <- train(classe ~ . -X, data = training, trControl = cvControl,  method = 'qda')
print(qdamod)
```


The test accuracy of **0.911** has exceeded our expectations and thus, we will not move on to more complex methods. 
```{r}
qdapred <- predict(qdamod, testing)
qdaconf <- table(testing$classe,qdapred)
print(qdaconf)
sum(diag(qdaconf)/sum(qdaconf))
```

Hence, the QDA model will be used for the validation. This produces the answer for the validation set, which scored a 100% accuracy on the first attempt(on the Quiz). The answer will however, not be shown due to the Coursera Honor code.
```{r}
qdavalid <- predict(qdamod, validation)
```
